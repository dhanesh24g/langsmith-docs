Evaluation Concepts
===================

High-quality evaluations are essential for refining, testing, and iterating AI applications. Meaningful evaluations make it easier to tailor prompts, choose models, experiment with new architectures, and confirm that your deployed applications continue to function as intended. LangSmith is designed to simplify the process of constructing these effective evaluations.

This guide walks through LangSmith’s evaluation framework and the underlying concepts for evaluating AI applications. It explores:

• Datasets, which serve as test collections for your application’s inputs (and optionally reference outputs).  
• Evaluators, which are functions that measure how well your application’s outputs meet certain criteria.

Datasets
--------

A dataset is a curated collection of examples—each containing inputs and optional reference outputs—that you use to measure your application’s performance.

Illustration: Datasets consist of examples. Each example may include inputs, optional reference outputs, and metadata.

### Examples

Each example represents one test case and generally includes three parts. First, it has one or more inputs—organized in a dictionary—that your application receives during a run. Second, it may include reference outputs, also called target or gold-standard outputs. These reference outputs are typically reserved for evaluators (instead of being fed directly into your application). Lastly, you can attach metadata in a dictionary format to keep track of any descriptive notes or tags you want to associate with the example. This metadata can then be used to filter or slice your dataset when performing evaluations.

Illustration: An example has inputs, possible reference outputs, and optional metadata.

### Dataset Curation

When building datasets to represent your application’s use cases, there are a few methods you can follow.

Manually Curated Examples. This approach is a strong starting point if you already know the kinds of tasks your app needs to handle and what good outputs look like. Carefully selected examples can catch both typical cases and edge cases. Even a small set—perhaps 10 to 20 entries—can offer significant insights.

Historical Traces. Once your system is active in production, you can gather real-world runs to see how actual users interact with your application. You might pick out runs flagged by user complaints or poor ratings, examine cases where runtime anomalies occurred, or programmatically detect interesting patterns (such as users repeating themselves when the system didn’t address their query effectively).

Synthetic Generation. You can bolster your dataset by asking a language model to generate fresh test examples. This scales efficiently but works best if you have previously curated a small batch of high-quality examples for the model to emulate.

### Splits

Datasets in LangSmith can be partitioned into one or more splits. Splits enable you to separate your data in ways that help you run cost-effective experiments on a smaller slice while retaining more extensive tests for comprehensive evaluation. For instance, with a retrieval-augmented generation (RAG) system, you could divide data between factual queries and opinion-based queries, testing each category independently.

Learn more about how to create and manage dataset splits here:  
(/evaluation/how_to_guides/manage_datasets_in_application#create-and-manage-dataset-splits)

### Versions

Every time you modify a dataset—adding, editing, or removing examples—LangSmith automatically creates a new version. This versioning allows you to revisit or revert earlier dataset states, making it easier to keep track of your changes as your application evolves. You can label these versions with meaningful tags that denote specific milestones or stable states of the dataset. You can also run evaluations on specific dataset versions if you want to lock a particular set of tests into a continuous integration (CI) pipeline.  
More details on dataset versioning are available here:  
(/evaluation/how_to_guides/version_datasets)

Evaluators
----------

Evaluators are functions that assign one or more metrics to your application’s outputs. They provide “grades” indicating how closely the application’s outputs align with the desired criteria.

### Evaluator Inputs

Evaluators receive both the example (which supplies the input data and any reference outputs) and the run (the actual output produced by your application). The run may include the final output and any intermediate steps that occurred along the way, such as tool calls.

### Evaluator Outputs

Evaluators produce metrics in a dictionary or list of dictionaries. Typically, these metrics will have:  
• A “key” which names the metric.  
• A “score” or “value” that holds either a numeric measure or a categorical label.  
• An optional “comment” that explains how the evaluator arrived at the score or label.

### Defining Evaluators

You can define and run LangSmith evaluators in a variety of ways. You can write your own custom evaluators in Python or TypeScript or rely on built-in evaluators that come with LangSmith. Evaluation can be triggered through the LangSmith SDK (in Python or TypeScript), the Prompt Playground (a feature within LangSmith), or via automated rules you set up in your project.

### Evaluation Techniques

When building evaluators for large language model (LLM) applications, you can choose from several common strategies:

Human Review. You or your team members can manually examine outputs for correctness and user satisfaction. This direct feedback is crucial, particularly in the early stages of development. LangSmith Annotation Queues allow you to structure this process for efficiency, including permissions and guidelines.

Heuristic Checking. Basic, rule-based evaluators can check for empty responses, monitoring how long responses are, or ensuring that certain keywords appear or do not appear.

LLM-as-Judge. You can use a language model to evaluate or grade outputs. This approach often involves encoding your evaluation instructions in a prompt. It can be used in situations either with or without reference outputs.

Pairwise Comparisons. When you’re testing two versions of your application, you can have an evaluator decide which version performed better for a given example. This is often simpler for tasks like summarization, where “which is better?” is easier to judge than producing an absolute numeric performance score.

Experiment
----------

Every time you run your dataset’s inputs through your application, you’re effectively launching a new experiment. Using LangSmith, you can track every experiment linked to a dataset, making it simple to compare different versions of your application side by side. This comparison helps you detect regressions or measure gains accurately as you refine prompts, models, or other system components.

Illustration: Compare multiple experiments side by side to see changes in scores or outputs.

Annotation Queues
-----------------

Gathering real user input is a key part of refining your system. With annotation queues, you can sort runs into a review flow where human annotators examine outputs and assign feedback or corrective notes. These collected annotations can eventually form a dataset for future evaluations. In some cases, you might label only a sample of runs; in others, you may label them all. Annotation queues provide a structured environment for capturing this feedback while making it easy to keep track of who reviewed what.

To learn more about annotation queues and best practices for managing human feedback, see:  
(/evaluation/how_to_guides#annotation-queues-and-human-feedback)

Offline Evaluation
------------------

Offline evaluation is done on a static dataset rather than live end-user queries. It’s often the best way to verify changes to your model or your workflow prior to deployment, since you can test your system on curated or historical examples and measure the results in a controlled environment.

Illustration: Offline evaluations let you pass many curated or historical inputs into your application and systematically measure performance.

### Benchmarking

Benchmarking involves running your application against a carefully assembled dataset to compare one or more metrics. For instance, you might supply question-answer pairs and then measure semantic similarity between your application’s answers and the reference answers. Alternatively, you could rely on LLM-as-judge prompts to assess correctness or helpfulness. Because creating and maintaining large reference datasets can be expensive, teams often reserve thorough benchmarking for high-stakes or major version releases.

### Unit Tests

When dealing with LLMs, you can still implement traditional “unit tests” in your codebase. In many cases, these tests are rule-based checks that verify whether a generated response meets basic criteria—such as being valid JSON or avoiding empty outputs. Including them in your continuous integration (CI) pipeline can automate detection of small but critical issues any time your underlying system changes.

### Regression Tests

Regression tests examine how your system handles a known set of examples after an update. Suppose you tweak your prompt or switch to a new model. By re-running the same dataset and comparing the old outputs against the new outputs, you can quickly spot examples where performance has worsened. The LangSmith dashboard presents this visually, highlighting negative changes in red and improvements in green.

Illustration: Regression view highlights newly broken examples in red, improvements in green.

### Backtesting

Backtesting replays your stored production traces—queries from real user sessions—through a newer version of your system. By comparing real user interactions against the new model’s outputs, you get a clear idea of whether the next model release will benefit your user base before you adopt it in production.

### Pairwise Evaluation (Offline)

Offline pairwise evaluation directly compares outputs from two different system versions on the same collection of examples. Rather than trying to score a single run in isolation, you simply choose which of two outputs is superior. This approach is particularly helpful in tasks like summarization.

Online Evaluation
-----------------

Online evaluation continuously measures your application’s performance in a live setting. Rather than waiting until an offline batch test is complete, online evaluation monitors production runs in near real time, allowing you to detect errors or performance deterioration the moment they appear. This can be done with heuristic methods, reference-free LLM prompts that check for common failure modes, or any custom-coded logic you choose to deploy in production.

Illustration: Online evaluation actively checks real-time runs for undesired application outputs.

Application-Specific Techniques
-------------------------------

Below are a few evaluation strategies tailored to specific LLM application patterns.

### Agents

Autonomous LLM-driven agents combine an LLM for decision-making with tools for calls and memory for context. Each agent step typically involves the LLM deciding whether to invoke a tool, how to parse the user’s request, and what to do next based on prior steps.

Illustration: The agent uses an LLM to decide whether to call a tool and how.

You can evaluate agents by focusing on:

• Final Response: Assess whether the ultimate answer is correct or helpful, ignoring the chain of actions the agent took.  
• Single Step: Look at each decision independently. Did the agent choose the correct tool or produce the correct query at each stage?  
• Trajectory: Check whether the sequence of actions is logical. You could compare the agent’s chosen tools with a reference “ideal” list, or see if the agent’s overall plan leads to the correct outcome.

#### Evaluating an Agent’s Final Response

If your concern is whether the end result is correct, you can evaluate it just as you would any other LLM-generated answer. This method disregards intermediate steps, so it’s simpler to implement but doesn’t highlight at which point errors occur.

#### Evaluating a Single Step

An agent often makes multiple decisions in sequence. By evaluating each step individually, you can catch smaller mistakes immediately. This requires more granular data on which tool was chosen at each step and why, and makes data collection slightly more complex.

#### Evaluating an Agent’s Trajectory

With trajectory-based evaluation, you consider the entire path from start to finish. This might involve matching the agent’s tool usage and outputs against a known “correct” chain of thought or simply passing the full trace to an evaluator (human or model) for a holistic verdict. Trajectory evaluations provide the richest feedback but require more setup and careful dataset construction.

### Retrieval Augmented Generation (RAG)

A RAG system fetches relevant documents to feed to the LLM. This is useful in tasks such as question-answering, enterprise search, or knowledge-based chat experiences.

Comprehensive RAG details:  
https://github.com/langchain-ai/rag-from-scratch

#### Dataset

For RAG, your dataset generally consists of queries and possibly reference answers. If reference answers exist, you can compare generated answers to these references for offline evaluation. If no reference answers exist, you can still measure whether relevant documents were retrieved or ask an LLM-as-judge to check whether the answer is faithful to the retrieved passages.

#### Evaluator

Evaluators for RAG systems often revolve around factual accuracy and alignment with retrieved documents. You can assess how relevant the retrieved documents were and whether or not the final answer relies on accurate information. This can be done offline if reference answers are available, online if you want immediate monitoring in production, or through pairwise evaluations to compare different retrieval strategies.

### Summarization

When summarizing text, there isn’t always a single “correct” summary. Consequently, LLM-based evaluators are popular. The model can be asked to check clarity, factual accuracy, or faithfulness to the original text. You can conduct these evaluations offline on a curated set of source documents or run them online in near real time for user-generated inputs. Pairwise evaluation is also a common approach, since it may be easier to choose which of two summaries is better than to assign an absolute quality score to a single summary.

### Classification / Tagging

Classification tasks assign labels or tags to inputs. If you already have a labeled dataset, standard precision, recall, and accuracy metrics can be calculated. If labels are not available, you can use an LLM-as-judge to categorize inputs according to specified criteria and check for consistency.

When reference labels exist, you can build a custom evaluator that compares predictions to ground truth and produces numeric performance metrics. Without reference labels, you could still rely on carefully designed prompts that instruct your model to classify inputs appropriately. Pairwise comparisons can be beneficial if you are testing two different classification systems and want to see which approach yields more satisfactory labels according to certain guidelines.

All the techniques discussed—offline or online evaluation, pairwise comparisons, heuristic checks, and more—can help ensure your classification tasks remain reliable as your system evolves.