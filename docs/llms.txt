# LangSmith

## Tracing

[Observability Quick Start | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability): LLM should read this page when getting started with LangSmith observability, instrumenting an application to trace LLM calls, or tracing an entire application pipeline. Provides a step-by-step tutorial on installing dependencies, creating an API key, setting environment variables, defining a sample application, tracing just OpenAI calls or the entire application pipeline using decorators, and suggestions for next steps.

[Concepts | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/concepts): LLM should read this page when configuring an application to log traces to LangSmith, understanding the different data types and concepts used in LangSmith, or planning data storage and retention for traces. The page covers key concepts like Runs, Traces, Projects, Feedback, Tags, and Metadata, and explains data storage, retention, and deletion policies for traces in LangSmith.

[Observability how-to guides | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides): LLM should read this page when needing to add tracing to an LLM application, view and interact with traces, or configure alerts and automation rules. This page provides step-by-step guides on setting up tracing, integrating with various frameworks, configuring advanced tracing options, using the tracing UI, creating dashboards, setting up automation rules, and logging user feedback.

[Access the current run (span) within a traced function | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/access_current_span): LLM should read this page when trying to access the current run (span) within a traced function, when needing to extract UUIDs or tags from the current run, or when needing to access other information from the current run. This page explains how to access the current run (span) within a traced function using the get_current_run_tree/getCurrentRunTree function in the Python or TypeScript SDK, and provides examples for doing so.

[Add metadata and tags to traces | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/add_metadata_tags): LLM should read this page when adding metadata or tags to traces, understanding how to query traces by metadata/tags, or associating additional information with traces. The page explains how to add arbitrary metadata and tags to traces in Python and TypeScript, providing code examples for different methods like decorators, context managers, and wrapped clients.

[Annotate code for tracing | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code): LLM should read this page when: 1) Learning how to log traces to LangSmith 2) Annotating code for tracing 3) Ensuring traces are submitted before application exit 'This page covers several approaches to annotate code for logging traces to LangSmith, including using decorators/wrappers, context managers, wrapping the OpenAI client, and using the RunTree API. It also discusses ensuring all traces are submitted before application exit.'

[Calculate token-based costs for traces | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/calculate_token_based_costs): LLM should read this page when calculating token-based costs for traces, setting up model pricing maps, or tracking costs for LLM invocations. This page explains how to set up a model pricing map in LangSmith and provide token counts for LLM runs to enable token-based cost calculation and tracking for traces.

[Compare traces | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/compare_traces): LLM should read this page when comparing traces in LangSmith, understanding the functionality of the trace comparison feature, and learning how to use the trace comparison view. This page explains how to access the trace comparison view in LangSmith, select traces for comparison, and navigate the side-by-side comparison of traces.

[Create dashboards | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/dashboards): LLM should read this page when creating custom dashboards for monitoring metrics, visualizing agent decisions at specific nodes, and comparing performance across models or configurations. This page covers creating and configuring dashboards with charts, applying filters, comparing metrics or data series within charts, managing charts, and using expanded chart views. It also provides a user journey example for visualizing agent decisions at a node.

[[Beta] Bulk Exporting Trace Data | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/data_export): LLM should read this page when needing to export trace data from LangSmith for offline analysis, needing to import trace data into other data analysis tools, or troubleshooting issues with data exports. This page covers how to bulk export trace data from LangSmith to an S3 bucket, how to monitor and manage export jobs, how to partition and import exported data into tools like BigQuery and Snowflake, and how to troubleshoot common errors during data exports.

[Implement distributed tracing | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/distributed_tracing): LLM should read this page when tracing requests across multiple services, implementing distributed tracing, or propagating trace context across boundaries. Covers setting up distributed tracing in Python and TypeScript, including examples for propagating trace context via headers and continuing traces across client-server boundaries.

[Query traces | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/export_traces): LLM should read this page when: 1) Querying traces based on specific conditions (e.g., run name, feedback score, latency, tokens, errors, timestamps, tags) 2) Analyzing trace data in advanced scenarios (e.g., flattened trace view, retriever I/O) 3) Exporting trace data for further analysis 'This page provides detailed guidance on querying LangSmith traces using filter arguments, query language syntax, and advanced techniques like exporting flattened trace views or retriever I/O. It covers various use cases for filtering traces based on run properties, metadata, feedback, latency, tokens, errors, timestamps, tags, and combining multiple conditions.'

[Filter traces in the application | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application): LLM should read this page when filtering traces in the LangSmith application, constructing complex trace filters, and saving/managing custom trace filters. This page provides a comprehensive guide on how to filter traces in the LangSmith application, including creating filters for intermediate runs, filtering based on inputs/outputs or key-value pairs, negative filtering, saving and managing custom filters, copying filters, filtering within the trace view, manually specifying raw queries, and using AI-generated queries.

[Log custom LLM traces | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/log_llm_trace): LLM should read this page when logging traces for custom LLM models, using streaming output, or providing manual token counts. Provides guidelines for logging LLM traces in the expected format for chat and instruct-style models, handling streaming outputs, and manually providing token usage data.

[Log multimodal traces | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/log_multimodal_traces): LLM should read this page when logging multimodal traces with images, using LangSmith for tracing multimodal LLM runs, working with LangChain or other LLM libraries to log image data This page explains how to log images as part of multimodal traces in LangSmith, including code examples in Python and TypeScript for using the wrap_openai/wrapOpenAI functions to pass image URLs or base64 data to the OpenAI API. It shows how the images are rendered in the LangSmith UI trace.

[Log retriever traces | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/log_retriever_trace): LLM should read this page when logging retriever traces in an application, understanding how to format retriever data for rendering, understanding how retriever traces are displayed in LangSmith This page explains how to log and format retriever traces in LangSmith, including annotating the retriever step, returning a list of documents with specific keys, and how retriever traces are rendered.

[Log traces to specific project | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/log_traces_to_project): LLM should read this page when needing to log traces to a specific project, changing the destination project statically or dynamically. This page explains how to set the project name for traces through environment variables or at runtime, with code examples in Python and TypeScript.

[Prevent logging of sensitive data in traces | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs): LLM should read this page when: 1) Needing to prevent sensitive data from being logged in application traces 2) Requiring customization of input/output masking for specific functions Explains how to hide inputs and outputs entirely, use regex patterns or third-party services for rule-based masking, and process inputs/outputs for individual traced functions.

[Troubleshoot trace nesting | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/nest_traces): LLM should read this page when troubleshooting trace nesting issues, when encountering split or disconnected traces, or when working with asyncio, threading, or context propagation in Python. Provides guidance on resolving trace nesting problems caused by context propagation issues with asyncio and threading in Python, offering solutions like upgrading Python version, manual context propagation, and using LangSmith's ContextThreadPoolExecutor.

[Set up online evaluations | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations): LLM should read this page when setting up online evaluations, configuring LLM or custom code evaluators, and testing evaluation functions. This page explains how to configure online evaluations on LangSmith, including setting up LLM-based evaluators with prompts and custom code evaluators with provided guidelines and restrictions.

[How to print detailed logs (Python SDK) | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/output_detailed_logs): LLM should read this page when needing to troubleshoot logging issues, adjusting log verbosity levels, or configuring logging for Python environments like Jupyter notebooks. Covers enabling Python's built-in logging mechanism, increasing logging verbosity for debugging, and configuring logging specifically for the LangSmith package.

[Set up automation rules | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/rules): LLM should read this page when setting up automation rules for LLM applications, viewing logs for automation rules, or understanding data retention This page explains how to create automation rules in LangSmith to trigger actions like online evaluations, adding traces to datasets, triggering webhooks, and extending data retention based on filters. It also covers viewing logs for automation rules.

[Set a sampling rate for traces | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/sample_traces): LLM should read this page when configuring sampling rate for traces, troubleshooting trace logging, optimizing performance. Explains how to set a sampling rate for traces logged to LangSmith, using the LANGSMITH_TRACING_SAMPLING_RATE environment variable to control the percentage of traces logged.

[Trace JS functions in serverless environments | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/serverless_environments): LLM should read this page when tracing JavaScript functions in serverless environments, ensuring tracing data is flushed before function completion, handling high concurrency scenarios. This page covers how to trace JavaScript functions in serverless environments using the LangSmith JS SDK, including setting environment variables, awaiting pending trace batches, handling rate limits at high concurrency, and manually flushing tracing data.

[Share or unshare a trace publicly | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/share_trace): LLM should read this page when sharing traces publicly, unsharing traces, or managing publicly shared traces. Covers how to share or unshare a trace publicly on LangSmith, including steps for sharing, unsharing from the trace view or list, and ensuring no sensitive data is exposed.

[Set up threads | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/threads): LLM should read this page when it needs to track multi-turn conversations or chatbot-like interactions, it needs to group related traces into threads, it needs to continue an existing conversation thread. This page explains how to associate traces together into threads using metadata keys like 'session_id' or 'thread_id', provides code examples for setting up threads in Python and TypeScript, and shows how to view conversation histories grouped into threads in the LangSmith UI.

[Toubleshooting variable caching | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/toubleshooting_variable_caching): LLM should read this page when troubleshooting issues with variable caching, when tracing data is not appearing as expected, or when traces are being logged to the wrong project/workspace. Provides steps to verify environment variables, clear the cache, and reload environment variables to resolve issues related to variable caching and tracing in LangSmith.

[Trace generator functions | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/trace_generator_functions): LLM should read this page when tracing generator functions, streaming LLM outputs, or customizing output aggregation. Explains how to trace generator and async generator functions, stream outputs to minimize latency, and aggregate streamed outputs into desired formats before tracing them.

[Trace LangChain with OpenTelemetry | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel): LLM should read this page when integrating LangChain with OpenTelemetry for tracing, setting up distributed tracing with LangChain, or sending traces to alternate observability providers. The page explains how to enable OpenTelemetry integration in LangChain, create traced applications, view traces in LangSmith, set up distributed tracing with context propagation, configure alternate OTLP endpoints, and use the OpenTelemetry Collector for fan-out tracing.

[Trace using the LangSmith REST API | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_api): LLM should read this page when tracing LLM applications using the LangSmith REST API, ingesting multiple traces in batches, or serializing runs for multipart requests. This page provides guidance on tracing requests using the LangSmith REST API, including basic tracing, batch ingestion of traces, and serializing runs for multipart requests.

[Trace with Instructor (Python only) | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_instructor): LLM should read this page when tracing Instructor Python code with LangSmith, using Instructor with OpenAI API, tracing nested Instructor function calls Provides guidance on setting up LangSmith tracing for Instructor, a Python library for generating structured outputs from LLMs, including wrapping OpenAI client, annotating functions with @traceable decorator

[Trace with LangChain (Python and JS/TS) | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain): LLM should read this page when: 1) Integrating LangChain with LangSmith for tracing and observability 2) Logging traces with metadata, tags, and custom run/project names 3) Accessing run IDs and ensuring traces are submitted before exiting 'Guide for tracing LangChain with LangSmith for observability. Covers environment setup, logging traces, configuring metadata/tags/names, accessing run IDs, and distributed tracing. Also covers interoperability between LangChain and LangSmith SDK.'

[Trace with LangGraph (Python and JS/TS) | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph): LLM should read this page when needing to trace LangChain and LangGraph workflows, needing to configure tracing for custom functions in those workflows Explains how to set up tracing for LangChain/LangGraph workflows using environment variables, tracing built-in LangChain modules, wrapping custom functions/SDKs with LangSmith decorators to trace

[Trace with OpenAI Agents SDK | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk): LLM should read this page when integrating the OpenAI Agents SDK with LangSmith, tracing agent execution flows, or logging agent traces to LangSmith. This page provides instructions for installing the LangSmith library with OpenAI Agents support, and a code example demonstrating how to use the OpenAIAgentsTracingProcessor class to trace and log agent execution flows to LangSmith.

[Trace with OpenTelemetry | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry): LLM should read this page when tracing AI/LLM applications using OpenTelemetry, using the Traceloop SDK, or using the Arize SDK. This page covers how to set up and log traces from OpenTelemetry-based clients, the Traceloop SDK, and the Arize SDK to LangSmith for observability.

[Trace with the Vercel AI SDK (JS/TS only) | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk): LLM should read this page when needing to trace runs from the Vercel AI SDK, needing to customize tracing settings for Vercel AI SDK runs, or needing to set up tracing for different environments like Next.js, Node.js, Cloudflare Workers, etc. This page provides a guide on how to use LangSmith to trace runs from the Vercel AI SDK, including installation, environment configuration, logging traces, customizing run names and IDs, nesting runs, using a custom LangSmith client, debugging, adding metadata, and a deprecated method for older versions.

[Trace without setting environment variables | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/trace_without_env_vars): LLM should read this page when needing to trace code without setting environment variables, when needing to programmatically set tracing configuration. This page covers how to enable or disable tracing without setting environment variables, using the tracing_context context manager in Python and passing configuration to the traceable decorator in TypeScript.

[Upload files with traces | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/upload_files_with_traces): LLM should read this page when uploading files with traces, working with multimodal inputs or outputs, or tracing LLM pipelines that involve files. This page explains how to upload binary files like images, audio, videos, PDFs, and CSVs along with traces using the Python and TypeScript SDKs of LangSmith.

[Use monitoring charts | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/use_monitoring_charts): LLM should read this page when needing to analyze monitoring data for LLM applications, view data over different time periods, or drill down into specific subsets of traces. This page explains how to use monitoring charts in LangSmith to visualize metrics, change time periods, slice data by metadata/tags, and drill into specific subsets of traces.

[Set up webhook notifications for rules | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/webhooks): LLM should read this page when setting up webhook notifications for automation rules, integrating with third-party services, or building custom event handling workflows. This page explains how to configure webhook URLs to receive notifications when LangSmith automation rules are triggered, including details on the webhook payload structure, security considerations, and an example implementation using Modal.

[Observability tutorials | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/tutorials): LLM should read this page when getting started with observability for LLM applications, learning about tracing LLM workflows, or exploring different tracing integrations. The page provides an overview of observability tutorials, including a quick start guide, in-depth tutorials on adding observability to LLM applications, and a list of how-to guides covering various observability-related topics.

[Add observability to your LLM application | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/tutorials/observability): LLM should read this page when developing an LLM application, adding observability to an existing LLM application, or integrating LangSmith's observability features into their application. This page provides a tutorial on adding observability to an LLM application using LangSmith, covering prototyping, beta testing, and production stages.

## Evaluation

[Evaluation Quick Start | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/evaluation): LLM should read this page when learning how to set up LangSmith evaluations, creating evaluation datasets, or implementing LLM-based evaluators. This page provides a step-by-step quick start guide for LangSmith's evaluation capabilities, covering installation, API key setup, dataset creation, defining evaluation targets, creating evaluators, and running evaluations in both Python and TypeScript.

[Evaluation concepts | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/evaluation/concepts): LLM should read this page when wanting to understand LangSmith evaluation concepts, implementing evaluation strategies for LLM applications, or choosing appropriate metrics for different AI application types. This page covers LangSmith's evaluation framework including datasets, evaluators (human, heuristic, LLM-as-judge, pairwise), experiments, annotation queues, offline/online evaluation approaches, testing methodologies, and application-specific evaluation techniques for agents, RAG, summarization, and classification.

[Evaluation how-to guides | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/evaluation/how_to_guides): LLM should read this page when looking for evaluation methods in LangSmith, needing to set up offline/online evaluations, or managing datasets for AI application testing. The page provides comprehensive how-to guides for LangSmith's evaluation features, covering offline evaluation setup, evaluator configuration, testing integration, online evaluation, experiment analysis, dataset management, and human feedback collection systems.

[Analyze a single experiment | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment): LLM should read this page when analyzing experiment results in LangSmith, troubleshooting evaluation metrics, or learning how to interpret experiment visualizations. This page explains how to analyze experiment results in LangSmith including: navigating the experiment view, using heatmap visualization, sorting/filtering results, viewing different table formats (compact/full/diff), accessing traces, examining evaluator runs, grouping by metadata, working with repetitions, and comparing experiments.

[Annotate traces and runs inline | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline): LLM should read this page when needing to annotate traces in LangSmith, adding manual feedback to application traces, or setting up inline annotation workflows. This page explains how to manually annotate traces in LangSmith by adding feedback, comments, and scores to any run in a trace, including intermediate spans, using the Annotate feature in the trace view.

[Use annotation queues | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues): LLM should read this page when needing to understand annotation queues in LangSmith, setting up human-in-the-loop feedback processes, or organizing systematic review of model outputs. This page explains how to create and use annotation queues in LangSmith, including creating queues with rubrics, assigning runs to queues, configuring multi-annotator workflows with reservations, and reviewing runs within a queue interface with keyboard shortcuts.

[How to run an evaluation asynchronously | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/evaluation/how_to_guides/async): LLM should read this page when needing to implement asynchronous evaluations in Python, handling concurrent evaluation requests, or working with async functions in LangSmith. This page explains how to use the `aevaluate()` function in LangSmith's Python SDK to run evaluations asynchronously, with sample code showing async function creation, handling concurrency, and integration with datasets for evaluation.

[Log user feedback | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback): LLM should read this page when needing to implement user feedback collection for LLM applications or wanting to integrate trace-based feedback with LangSmith. This page explains how to log user feedback to LangSmith traces using both Python and TypeScript, allowing developers to attach feedback scores and comments to any part of an application trace for analysis and evaluation.

[How to audit evaluator scores | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores): LLM should read this page when auditing AI evaluation scores, needing to correct LLM judge assessments, or implementing score correction workflows. This page explains how to audit and correct LLM-as-judge evaluator scores in LangSmith using three methods: through the comparison view UI, the runs table UI, or programmatically via the SDK (Python or TypeScript) using the update_feedback function.

[How to bind an evaluator to a dataset in the UI | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset): LLM should read this page when learning how to configure evaluators for datasets in LangSmith UI, when setting up automatic evaluation, or when creating custom evaluation functions. This page provides step-by-step instructions for binding evaluators to datasets in the LangSmith UI, covering both LLM-as-judge evaluators and custom code evaluators, with examples of implementation, restrictions, and visualization of evaluation results.

[How to compare experiment results | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results): LLM should read this page when comparing experiment results in LangSmith, analyzing differences between LLM application iterations, or interpreting metrics from multiple evaluations. This page provides a comprehensive guide to using LangSmith's experiment comparison view, including opening the comparison view, adjusting table displays, analyzing regressions/improvements, filtering results, setting baseline experiments, viewing traces and detailed views, and creating summary charts with customizable metadata labels.

[How to create few-shot evaluators | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators): LLM should read this page when creating few-shot evaluators in LangSmith, setting up corrective learning for LLM evaluators, or managing evaluation feedback cycles. This guide explains how to create few-shot evaluators that improve over time using human corrections as examples, including how to set up mustache-template prompts with the {{Few-shot examples}} variable, make corrections that populate into a dedicated dataset, and access/edit the few-shot examples dataset.

[How to define a custom evaluator | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator): LLM should read this page when needing to create custom evaluators for LangSmith, understanding how to return different types of metrics, or implementing LLM-as-judge evaluators. The page explains how to define custom evaluator functions in LangSmith for both Python and TypeScript, including the required argument specifications, supported return types, and examples of different evaluation approaches such as exact matching, conciseness scoring, and using LLMs as judges.

[How to evaluate on a split / filtered view of a dataset | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset): LLM should read this page when evaluating subsets of datasets, applying filters to datasets, or running evaluations on specific dataset splits. This page explains how to evaluate filtered views of datasets by using list_examples with metadata filters and how to run evaluations on specific dataset splits (like "test" or "training") using the LangSmith evaluation framework in both Python and TypeScript.

[How to evaluate on a specific dataset version | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version): LLM should read this page when needing to evaluate models on specific dataset versions, managing dataset versioning in LangSmith, or implementing version-specific evaluations. This page explains how to evaluate on a specific dataset version by using the list_examples/listExamples method with the as_of/asOf parameter to specify which version to use during evaluation.

[How to define a target function to evaluate | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target): LLM should read this page when needing to configure a target function for evaluation in LangSmith, implementing evaluations for LLM applications, or setting up testing frameworks for AI components. This page explains how to define target functions for LangSmith evaluations, including the function signature requirements, examples for single LLM calls, non-LLM components, and complete applications/agents with code samples in Python and TypeScript.

[How to download experiment results as a CSV | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv): LLM should read this page when needing to download experiment data from LangSmith, wanting to export evaluation results, or analyzing LangSmith experiment outcomes offline. This page explains how to download experiment results as a CSV file from LangSmith by clicking the download icon at the top of the experiment view, located to the left of the "Compact" toggle.

[How to evaluate an existing experiment (Python only) | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment): LLM should read this page when needing to evaluate existing experiments or add evaluation metrics to previously run experiments. This page explains how to use the Python SDK to apply evaluators to existing experiments using the evaluate() method with an experiment name/ID instead of a target function.

[How to run an evaluation | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application): LLM should read this page when learning how to evaluate LLM applications, setting up evaluation pipelines, or debugging model performance. This page explains the process of running evaluations in LangSmith, covering how to define an application for testing, create datasets with labeled examples, define custom evaluators, run evaluation jobs with the evaluate() method, and analyze results in the LangSmith UI.

[How to evaluate an application's intermediate steps | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps): LLM should read this page when evaluating intermediate steps of AI applications, working with RAG pipelines, or creating custom evaluators for complex systems. This guide explains how to evaluate intermediate steps in LLM applications, with examples using a Wikipedia-based RAG pipeline, creating custom evaluators for retrieval relevance and hallucination detection, and implementing evaluation through LangSmith with both Python and TypeScript code samples.

[How to run pairwise evaluations | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise): LLM should read this page when comparing outputs from multiple experiments against each other, setting up pairwise evaluations, or implementing LLM-as-judge comparisons. This page explains how to run pairwise evaluations in LangSmith, covering the evaluate() function arguments, defining custom pairwise evaluators, handling evaluator inputs/outputs, running evaluations, and viewing results in the LangSmith UI.

[Run an evaluation with large file inputs | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments): LLM should read this page when working with multimodal file inputs, processing large attachments in LangSmith, or creating evaluations with file uploads. This page explains how to run LangSmith evaluations with large file attachments, including creating examples with attachments via SDK or UI, defining target functions that use these files, building custom evaluators that process attachments, and managing attachments through updates and versioning.

[How to export filtered traces from experiment to dataset | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset): LLM should read this page when needing to save specific evaluation results as datasets, filtering experiment traces by evaluation criteria, or creating new datasets from filtered runs. This guide demonstrates how to export filtered traces from a LangSmith experiment to a dataset by navigating to experiment traces, applying filters based on evaluation criteria, selecting desired runs, and using the "Add to Dataset" feature.

[How to fetch performance metrics for an experiment | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/evaluation/how_to_guides/fetch_perf_metrics_experiment): LLM should read this page when needing to extract performance metrics from LangSmith experiments or when analyzing evaluation results programmatically. This page explains how to fetch and interpret experiment performance metrics using the LangSmith SDK, including details on latency, token usage, costs, feedback statistics, and error rates, with code examples in both Python and TypeScript.

[How to filter experiments in the UI | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui): LLM should read this page when needing to filter LangSmith experiments, looking for ways to organize evaluation results, or wanting to compare specific experiment metrics in the UI. This page explains how to filter experiments in LangSmith's UI by adding metadata to experiments during creation and using the filtering interface to narrow down results by model provider, prompt type, feedback scores, and other criteria.

[Dynamic few shot example selection | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection): LLM should read this page when looking for dynamic few-shot example selection, searching datasets based on input similarity, or implementing retrieval-based prompting. This page explains how to configure LangSmith datasets for dynamic few-shot example selection, including prerequisites (paid team plan, KV store data type), indexing datasets for search, testing search quality in the playground, and implementing this feature in applications using code snippets in Python and TypeScript.

[How to evaluate a langchain runnable | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable): LLM should read this page when evaluating LangChain Runnable objects, setting up evaluation pipelines, or using LangSmith for model testing. This page explains how to evaluate LangChain Runnable objects (like chains, retrievers, and models) using LangSmith, with code examples in Python and TypeScript showing how to define a chain, create evaluation metrics, and run evaluations against datasets.

[How to evaluate a langgraph graph | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph): LLM should read this page when evaluating langgraph applications, creating evaluators for agent workflows, or running evaluations on graph nodes. This guide explains how to evaluate langgraph graphs, covering end-to-end evaluations, evaluating intermediate steps, and testing individual nodes with examples for creating datasets, defining evaluators, and analyzing results.

[How to define an LLM-as-a-judge evaluator | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge): LLM should read this page when implementing LLM-as-a-judge evaluators, setting up evaluation systems for conversational AI, or creating custom evaluators for language models. The page explains how to define an LLM-as-a-judge evaluator using the LangSmith SDK, including custom evaluator implementation with code examples that use OpenAI GPT models to evaluate the consistency of reasoning in AI-generated responses.

[How to run an evaluation locally (beta, Python only) | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/evaluation/how_to_guides/local): LLM should read this page when learning how to run evaluations locally without uploading results to LangSmith, when testing prompts quickly, or when validating target/evaluator functions. The page explains how to use the Python SDK's evaluate() function with upload_results=False parameter to run evaluations locally, with an example showing how to define evaluators, test a chatbot on a dataset, and analyze results locally with pandas.

[How to manage datasets in the UI | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application): LLM should read this page when needing to create or manage datasets in LangSmith UI, looking for ways to add examples to datasets, or wanting to organize evaluation data. This guide covers dataset creation (from CSV or empty), adding examples (from traced runs, annotation queues, UI input, or synthetic generation), exporting datasets, creating dataset splits, editing example metadata, and filtering examples in the LangSmith UI.

[How to manage datasets programmatically | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically): LLM should read this page when needing to programmatically create, manage, query, or update datasets in LangSmith, or when working with dataset examples in Python or TypeScript. This page documents how to use the LangSmith SDK to: create datasets from lists, traces, CSV files, or pandas DataFrames; fetch and filter datasets; query examples using various criteria; and update examples individually or in bulk.

[How to return categorical vs numerical metrics | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type): LLM should read this page when implementing evaluators that need to distinguish between metric types, when creating custom metrics in LangSmith, or when configuring how metric results are displayed. This page explains how to return categorical vs numerical metrics in LangSmith custom evaluators, showing code examples in Python and TypeScript for both metric types and their proper return formats.

[How to return multiple scores in one evaluator | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores): LLM should read this page when implementing multiple evaluation metrics in a single evaluator, needing to optimize cost/time with LLM judges, or working with custom evaluators in LangSmith. The page explains how to return multiple scores from a single evaluator function in both Python and TypeScript, with code examples showing how to structure the return value as a list of dictionaries containing metric names and scores.

[How to use prebuilt evaluators | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators): LLM should read this page when needing to use prebuilt evaluators in LangSmith, implementing LLM-as-a-judge for evaluation, or integrating evaluation into testing frameworks. This page explains how to use the openevals package for ready-made LangSmith evaluators, covering setup requirements, implementation with Python and TypeScript, and integration with pytest/Vitest for running evaluations that automatically log results as feedback.

[How to run evals with pytest (beta) | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest): LLM should read this page when setting up evaluation with pytest, integrating LangSmith with Python testing, or implementing test caching for LLM applications. The page explains how to use LangSmith's pytest plugin for evaluation, covering installation, test definition, logging inputs/outputs/feedback, test suite organization, caching requests, rich terminal outputs, parameterization, and assertion utilities like the expect API.

[How to handle model rate limits | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting): LLM should read this page when handling rate limit errors in LangSmith evaluations, implementing throttling for LLM API calls, or optimizing concurrent model requests. This page covers three main approaches to handling model rate limits: using langchain RateLimiters to control request frequency, implementing retrying with exponential backoff, and limiting max_concurrency to reduce parallel API calls.

[Renaming an experiment | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment): LLM should read this page when needing to rename LangSmith experiments, understanding experiment name management, or finding UI navigation options for experiment renaming. This page explains two methods for renaming experiments in LangSmith: using the Playground interface where you can edit names in the table header, and using the pencil icon in the Experiments view. Experiment names must be unique per workspace.

[How to evaluate with repetitions | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition): LLM should read this page when configuring experiment repetitions, analyzing non-deterministic LLM outputs, or interpreting repetition results in LangSmith. This page explains how to configure multiple repetitions for evaluations to reduce noise in non-deterministic systems, set the num_repetitions parameter in evaluate functions, and view repetition results including averages and standard deviations in the LangSmith UI.

[How to use the REST API | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only): LLM should read this page when using the LangSmith REST API directly, implementing evaluations without SDKs, or working in a non-Python/JavaScript environment. The page explains how to use the LangSmith REST API to create datasets, run evaluations, and implement pairwise experiments, with complete code examples using Python's requests library.

[How to run an evaluation from the prompt playground | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground): LLM should read this page when running evaluations from the LangSmith prompt playground, testing prompts across multiple inputs without coding, or understanding how to create experiments in LangSmith UI. This page explains how to run evaluations using LangSmith's prompt playground: navigating to the playground, switching to a dataset, starting an experiment, viewing results, and adding evaluation scores by binding evaluators to datasets or using the SDK programmatically.

[Set up feedback criteria | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria): LLM should read this page when setting up feedback systems in LangSmith, configuring evaluation criteria, or creating custom feedback tags. This page explains how to create and configure feedback criteria in LangSmith, covering both continuous feedback (with min/max numerical values) and categorical feedback (with predefined categories mapped to scores).

[How to share or unshare a dataset publicly | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset): LLM should read this page when needing to share datasets publicly, managing dataset access permissions, or understanding public sharing implications in LangSmith. The page explains how to share and unshare datasets publicly in LangSmith, including using the Share button, accessing shared datasets via links, viewing permissions for shared datasets, and methods to unshare datasets through the UI.

[How to define a summary evaluator | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/evaluation/how_to_guides/summary): LLM should read this page when needing to create evaluators that run across entire experiments, designing summary metrics like F1 scores, or implementing experiment-level evaluation metrics. This page explains how to define summary evaluators in LangSmith for metrics that operate on entire experiments rather than individual runs, including function signature requirements, available arguments, and output formats with Python and TypeScript examples.

[How to upload experiments run outside of LangSmith with the REST API | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments): LLM should read this page when needing to upload experiments to LangSmith that were run outside the platform or when integrating external evaluation systems with LangSmith's visualization capabilities. This page explains how to use the REST API to upload externally-run experiments to LangSmith, including the request body schema, key considerations for dataset management, a working Python example using the requests library, and instructions for viewing uploaded experiments in the LangSmith UI.

[How to use off-the-shelf evaluators (Python only) | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old): LLM should read this page when needing to use pre-built evaluators in LangSmith with Python, understanding evaluation options for different types of responses, or configuring evaluator parameters. This page documents how to use LangChain's off-the-shelf evaluators in LangSmith including QA evaluators, criteria evaluators, labeled criteria evaluators, string/embedding distance metrics, using custom LLMs for evaluation, and handling multiple input/output fields.

[How to version a dataset | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets): LLM should read this page when needing to understand dataset versioning in LangSmith, managing dataset history, or tagging specific dataset versions. This page explains how LangSmith automatically creates new dataset versions when examples are added/updated/deleted, shows how to view past versions, and demonstrates how to tag versions with human-readable names like "prod" through both the UI and Python SDK.

[How to run evals with Vitest/Jest (beta) | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest): LLM should read this page when implementing testing for LLM applications in JavaScript/TypeScript or when setting up automated evaluation of language models with Vitest/Jest. This page provides a comprehensive guide to running evaluations with Vitest/Jest in LangSmith, including setup instructions for both frameworks, defining test suites, logging outputs, tracing feedback, parameterizing tests across multiple examples, and configuring test behavior with options for skipping, focusing, and dry-run mode.

[Evaluation tutorials | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/evaluation/tutorials): LLM should read this page when seeking evaluation tutorials for LLM applications, learning about assessment methodologies, or finding guides for testing specific AI systems. This page provides a collection of LangSmith evaluation tutorials covering how to evaluate chatbots, RAG applications, complex agents, ReAct agents with testing frameworks, and run backtests on agent versions.

[Evaluate a complex agent | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/agents): LLM should read this page when evaluating complex agents, understanding agent evaluation techniques, or implementing evaluation for multi-step reasoning systems. This page demonstrates how to build and evaluate a complex customer support agent with LangSmith, covering three key evaluation types: final response evaluation, trajectory evaluation, and single-step evaluation, with code examples for each approach.

[Run backtests on a new version of an agent | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/backtesting): LLM should read this page when testing new versions of AI agents, setting up backtesting workflows, or comparing model performance against historical data. This page explains how to run backtests on new agent versions using LangSmith, covering the process of converting production traces to datasets, defining evaluators, benchmarking systems, and analyzing comparative results to identify improvement opportunities before deployment.

[Evaluate a chatbot | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/evaluation): LLM should read this page when learning how to evaluate chatbots with LangSmith, setting up automated evaluation systems, or comparing model performances. This page provides a complete tutorial on evaluating a chatbot using LangSmith, covering dataset creation, metric definition, running evaluations with different models/prompts, comparing results, tracking performance over time, and setting up automated testing in CI/CD pipelines.

[Evaluate a RAG application | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/rag): LLM should read this page when evaluating RAG applications, implementing metrics for RAG evaluation, or designing comprehensive evaluation frameworks for retrieval-based systems. This tutorial demonstrates how to evaluate RAG applications using LangSmith, covering dataset creation, running evaluations, and implementing four key evaluation metrics: correctness (answer vs reference), relevance (response vs input), groundedness (response vs retrieved docs), and retrieval relevance (retrieved docs vs input).

[Running SWE-bench with LangSmith | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark): LLM should read this page when evaluating code agents on SWE-bench, implementing benchmarking for code generation, or setting up automated evaluation of coding tasks. This page provides a complete tutorial for running the SWE-bench benchmark with LangSmith, covering dataset loading, uploading to LangSmith, running a prediction function, evaluating code patches in Docker containers, and sending evaluation results back to LangSmith for analysis.

[Test a ReAct agent with Pytest/Vitest and LangSmith | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/testing): LLM should read this page when testing a ReAct agent using automated testing frameworks or integrating LangSmith with Pytest/Vitest/Jest. This page explains how to test LLM applications using LangSmith with popular testing frameworks, including setting up the environment, creating a stock information agent with tools, writing comprehensive tests for tool usage accuracy, and evaluating response groundedness with LLM-as-a-judge.

## Prompt Engineering

[Create a prompt | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/create_a_prompt): LLM should read this page when creating a new prompt, saving a prompt, or adding metadata to a prompt. This page provides instructions on how to create a new prompt using the LangSmith UI, save the prompt as private or public, and add metadata like descriptions and use cases.

[Run the playground against a custom LangServe model server | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_endpoint): LLM should read this page when considering how to run the LangSmith playground against a custom model server, when adding configurable fields to a custom model, and when using a custom model in the LangSmith playground. This page provides guidance on deploying a custom model server using LangServe, adding configurable fields to the model, and using the custom model in the LangSmith playground.

[Run the playground against an OpenAI-compliant model provider/proxy | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_openai_compliant_model): LLM should read this page when it needs to use a model that is compatible with the OpenAI API in the LangSmith playground, or when it needs to deploy and use a custom model in the LangSmith playground. This page explains how to deploy an OpenAI-compliant model provider/proxy, and how to use that model in the LangSmith playground by setting the OpenAI Proxy Provider option.

[LangChain Hub | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/langchain_hub): LLM should read this page when needing an overview of the LangChain Hub, wanting to search for existing prompts, or forking prompts to a personal organization. The page covers navigating the LangChain Hub section, which lists all publicly available prompts that can be searched, viewed in detail, run in the playground, and programmatically accessed via the SDK.

[Manage prompts programmatically | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically): LLM should read this page when managing prompts programmatically, pushing/pulling prompts to/from the LangChain Hub, or converting prompts to work with the OpenAI or Anthropic APIs directly. Provides instructions and code examples for installing packages, configuring environment variables, pushing and pulling prompts, using prompts without LangChain, and listing/deleting/liking prompts through the LangSmith SDK.

[Managing Prompt Settings | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/managing_model_configurations): LLM should read this page when managing prompt settings, creating/editing configurations, or setting default configurations. Covers managing model configurations, tool settings, prompt formatting, and extra parameters in the LangSmith playground.

[Open a prompt from a trace | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/open_a_prompt_from_a_trace): LLM should read this page when it needs to open a prompt from a trace for debugging or analysis, when it wants to link prompts used in code to the actual prompt definitions, when it needs to filter traces by prompt metadata This page explains how to find and open prompts from traces in the LangSmith UI, including showing the prompt link in traces, viewing prompt metadata, and filtering traces by prompt attributes

[Prompt Canvas | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_canvas): LLM should read this page when editing prompts, iterating on long prompts, or making stylistic/tonal changes to prompts The page describes the Prompt Canvas feature in LangSmith, which allows editing prompts with LLM assistance, quick actions to modify prompts, custom quick actions, diffing versions, and saving/using updated prompts.

[Prompt Tags | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_tags): LLM should read this page when trying to understand prompt tags in LangSmith, when managing different versions of prompts, when deploying prompts to different environments. This page covers an overview of prompt tags, how to create/move/delete tags, using tags in code, and common use cases like environment-specific tags and version control.

[Testing over a dataset | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/testing_over_dataset): LLM should read this page when testing prompts over a dataset, evaluating prompt performance, or running experiments with evaluators. This page explains how to test prompts over a dataset in the LangSmith Playground, select evaluators for the experiment, and run the experiment to evaluate prompt performance.

[Update a prompt | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/update_a_prompt): LLM should read this page when updating a prompt's description, use cases or content, or versioning a prompt. Covers how to edit prompt metadata, update prompt content in the playground, commit changes as new versions, and view commit history.

